{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "775c5af7-6c0e-426b-a8a8-2395872d3b7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Treat cast date column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9dd65054-808b-466d-a105-058d3a964ffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fe3315e-805f-4d2d-b9d3-e59e42fdcb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "a guy from a whatapp group:\n",
    "\n",
    "\n",
    "“\n",
    "Eu to tendo um problema em alterar o dado da tabela no databricks.... a tabela tem uma coluna chamada \"DATE\" e ela ta no formato string (com mês/dia/ano) eu quero mudar para formato data. Só que toda vez que faço isso a coluna inteira fica nula :/\n",
    "“\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f348c20-250c-497a-9299-5bcefa0e1257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Solution 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d1d24ac-5e6d-4f9b-ae7a-120d4d70204f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n|date      |id |\n+----------+---+\n|24/05/2025|1  |\n|31/01/2025|2  |\n|31/03/2025|3  |\n|28/02/2025|4  |\n|29/02/2025|5  |\n+----------+---+\n\nroot\n |-- date: string (nullable = true)\n |-- id: string (nullable = true)\n\n+----------+---+----------------+\n|date      |id |transformed_date|\n+----------+---+----------------+\n|24/05/2025|1  |2025-05-24      |\n|31/01/2025|2  |2025-01-31      |\n|31/03/2025|3  |2025-03-31      |\n|28/02/2025|4  |2025-02-28      |\n|29/02/2025|5  |null            |\n+----------+---+----------------+\n\nroot\n |-- date: string (nullable = true)\n |-- id: string (nullable = true)\n |-- transformed_date: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, when, to_date\n",
    "from pyspark.sql import DataFrame, Column, SparkSession\n",
    "\n",
    "spark: SparkSession = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Python Spark Date rule example\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "source_df: DataFrame = spark.createDataFrame(\n",
    "    [\n",
    "        (\"24/05/2025\", \"1\"),\n",
    "        (\"31/01/2025\", \"2\"),\n",
    "        (\"31/03/2025\", \"3\"),\n",
    "        (\"28/02/2025\", \"4\"),\n",
    "        (\"29/02/2025\", \"5\"), # must return NULL (None) value\n",
    "    ],\n",
    "    [\"date\", \"id\"]\n",
    ")\n",
    "\n",
    "source_df.show(10,False)\n",
    "source_df.printSchema()\n",
    "\n",
    "date_format: str = \"dd/MM/yyyy\"\n",
    "transformed_df: DataFrame = source_df.withColumn(\n",
    "    \"transformed_date\",\n",
    "    to_date(col(\"date\"), date_format)\n",
    ")\n",
    "\n",
    "transformed_df.show(10,False)\n",
    "transformed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33616915-9627-4c36-9f0b-3d108349f459",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 'Solution 2'\n",
    "The same solution as the previous one, but with:\n",
    "\n",
    "`.config(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")`\n",
    "\n",
    "We can see that the last three dates in source_df are null.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "591d30fa-78c0-4507-9e21-3f3e7abed429",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n|date      |id |\n+----------+---+\n|24/05/2025|1  |\n|31/01/2025|2  |\n|31/03/2025|3  |\n|28/02/2025|4  |\n|29/02/2025|5  |\n|24/06/25  |6  |\n|24/5/25   |7  |\n+----------+---+\n\nroot\n |-- date: string (nullable = true)\n |-- id: string (nullable = true)\n\n+----------+---+----------------+\n|date      |id |transformed_date|\n+----------+---+----------------+\n|24/05/2025|1  |2025-05-24      |\n|31/01/2025|2  |2025-01-31      |\n|31/03/2025|3  |2025-03-31      |\n|28/02/2025|4  |2025-02-28      |\n|29/02/2025|5  |null            |\n|24/06/25  |6  |null            |\n|24/5/25   |7  |null            |\n+----------+---+----------------+\n\nroot\n |-- date: string (nullable = true)\n |-- id: string (nullable = true)\n |-- transformed_date: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, when, to_date\n",
    "from pyspark.sql import DataFrame, Column, SparkSession\n",
    "\n",
    "spark: SparkSession = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Python Spark Date rule example\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "source_df: DataFrame = spark.createDataFrame(\n",
    "    [\n",
    "        (\"24/05/2025\", \"1\"),\n",
    "        (\"31/01/2025\", \"2\"),\n",
    "        (\"31/03/2025\", \"3\"),\n",
    "        (\"28/02/2025\", \"4\"),\n",
    "        (\"29/02/2025\", \"5\"), # must return NULL (None) value\n",
    "        (\"24/06/25\", \"6\"), \n",
    "        (\"24/5/25\", \"7\"),\n",
    "    ],\n",
    "    [\"date\", \"id\"]\n",
    ")\n",
    "\n",
    "source_df.show(10,False)\n",
    "source_df.printSchema()\n",
    "\n",
    "date_format: str = \"dd/MM/yyyy\"\n",
    "parsed_date_col: Column = to_date(col(\"date\"),date_format)\n",
    "\n",
    "transformed_df: DataFrame = source_df.withColumn(\n",
    "    \"transformed_date\",\n",
    "    to_date(col(\"date\"), date_format)\n",
    ")\n",
    "\n",
    "transformed_df.show(10,False)\n",
    "transformed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a7b5ad1-d396-48e0-be55-083b92c41860",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Solution 3\n",
    "Now we check the date formats using the configuration below\n",
    "```\n",
    ".config(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a25b73-9525-4a56-ade1-c787d34f9514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n|date      |id |\n+----------+---+\n|24/05/2025|1  |\n|31/01/2025|2  |\n|31/03/2025|3  |\n|28/02/2025|4  |\n|29/02/2025|5  |\n|24/06/25  |6  |\n|24/5/25   |7  |\n+----------+---+\n\nroot\n |-- date: string (nullable = true)\n |-- id: string (nullable = true)\n\nCASE\n        WHEN date_format(to_date(date, 'dd/MM/yyyy'), 'dd/MM/yyyy') = date\n        THEN to_date(date, 'dd/MM/yyyy')\n        WHEN date_format(to_date(date, 'dd/MM/yy'), 'dd/MM/yy') = date\n        THEN to_date(date, 'dd/MM/yy')\n        WHEN date_format(to_date(date, 'd/M/yy'), 'd/M/yy') = date\n        THEN to_date(date, 'd/M/yy')\n    ELSE NULL\nEND\n+----------+---+----------------+\n|date      |id |date_transformed|\n+----------+---+----------------+\n|24/05/2025|1  |2025-05-24      |\n|31/01/2025|2  |2025-01-31      |\n|31/03/2025|3  |2025-03-31      |\n|28/02/2025|4  |2025-02-28      |\n|29/02/2025|5  |null            |\n|24/06/25  |6  |2025-06-24      |\n|24/5/25   |7  |2025-05-24      |\n+----------+---+----------------+\n\nroot\n |-- date: string (nullable = true)\n |-- id: string (nullable = true)\n |-- date_transformed: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit, when, date_format, to_date, expr\n",
    "from pyspark.sql import DataFrame, Column, SparkSession\n",
    "from typing import List\n",
    "from functools import reduce\n",
    "\n",
    "spark: SparkSession = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"Python Spark Date rule example\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "mock_df: DataFrame = spark.createDataFrame(\n",
    "    [\n",
    "        (\"24/05/2025\", \"1\"),\n",
    "        (\"31/01/2025\", \"2\"),\n",
    "        (\"31/03/2025\", \"3\"),\n",
    "        (\"28/02/2025\", \"4\"),\n",
    "        (\"29/02/2025\", \"5\"), # must return NULL (None) value\n",
    "        (\"24/06/25\", \"6\"), \n",
    "        (\"24/5/25\", \"7\"),\n",
    "    ],\n",
    "    [\"date\", \"id\"]\n",
    ")\n",
    "mock_df.show(10,False)\n",
    "mock_df.printSchema()\n",
    "\n",
    "date_formats: List[str] = [\n",
    "    \"dd/MM/yyyy\",\n",
    "    \"dd/MM/yy\",\n",
    "    \"d/M/yy\"\n",
    "    ]\n",
    "\n",
    "date_column_name: str = \"date\"\n",
    "\n",
    "sql_case_expr: str = reduce(\n",
    "    lambda acc, fmt: acc + f\"\"\"\n",
    "        WHEN date_format(to_date({date_column_name}, '{fmt}'), '{fmt}') = {date_column_name}\n",
    "        THEN to_date({date_column_name}, '{fmt}')\"\"\",\n",
    "    date_formats,\n",
    "    \"CASE\"\n",
    ")\n",
    "\n",
    "sql_case_expr += f\"\"\"\n",
    "    ELSE NULL\n",
    "END\"\"\"\n",
    "\n",
    "print(sql_case_expr)\n",
    "\n",
    "transformed_df: DataFrame = mock_df.withColumn(\n",
    "    \"date_transformed\", expr(sql_case_expr)\n",
    ")\n",
    "\n",
    "transformed_df.show(10,False)\n",
    "transformed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b05c28ec-bf61-477a-9060-9db18665c996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ce0f6d2-4c19-4849-9fc6-1865f24a37ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Note - TimeParserPolicy](https://docs.databricks.com/aws/en/sql/language-manual/parameters/legacy_time_parser_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58d1dc02-889e-4975-8846-55f2ef1593e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### [Notes - Datetime Patterns for Formatting and Parsing](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "239498ca-a42a-462b-878a-47fbd4d8b87f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "There are several common scenarios for datetime usage in Spark:\n",
    "\n",
    "CSV/JSON datasources use the pattern string for parsing and formatting datetime content.\n",
    "\n",
    "Datetime functions related to convert StringType to/from DateType or TimestampType. For example, unix_timestamp, date_format, to_unix_timestamp, from_unixtime, to_date, to_timestamp, from_utc_timestamp, to_utc_timestamp, etc.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "treat cast date column",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}